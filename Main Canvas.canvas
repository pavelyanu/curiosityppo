{
	"nodes":[
		{"id":"9cd2b466d8460512","type":"file","file":"Main.md","x":-480,"y":186,"width":800,"height":2254},
		{"id":"2dac947dacde746e","type":"file","file":"Papers/Exploration Papers.md","x":480,"y":956,"width":531,"height":714,"color":"4"},
		{"id":"8ababad3874256dd","type":"file","file":"Papers/Misc Papers.md","x":-1160,"y":1153,"width":531,"height":320,"color":"6"},
		{"id":"f6e57be8870aac37","type":"file","file":"Papers/Implementation Papers.md","x":-345,"y":-280,"width":531,"height":340,"color":"1"},
		{"id":"9ffa4fcedb0b71ef","type":"file","file":"Papers/Exploration/Exploration by Random Network Distillation.md","x":1160,"y":-80,"width":400,"height":374,"color":"4"},
		{"id":"c98890147ada15f4","type":"text","text":"Seems implementable via simple learner and model augmentations\n","x":1720,"y":67,"width":335,"height":81},
		{"id":"123abf0f0efe5e60","type":"file","file":"Papers/Exploration/Never Give Up.md","x":1160,"y":420,"width":369,"height":380,"color":"4"},
		{"id":"95d8f63bf3911f07","type":"file","file":"Papers/Exploration/Curiosity-driven Exploration by Self-supervised Prediction.md","x":1160,"y":1313,"width":400,"height":400,"color":"4"},
		{"id":"08a8686e3b86fc83","type":"text","text":"Life Long curiosity","x":1688,"y":420,"width":250,"height":60},
		{"id":"7c40b754c960f28d","type":"text","text":"Episodic curiosity","x":1688,"y":515,"width":250,"height":60},
		{"id":"b02a599de63de2be","type":"file","file":"Figures/Never Give Up Figure 1.png","x":1688,"y":680,"width":1033,"height":500},
		{"id":"172f0331c219c8d3","type":"file","file":"Papers/Nethack Papers.md","x":-345,"y":2580,"width":531,"height":260,"color":"5"},
		{"id":"0942876d50f7bde2","type":"file","file":"Papers/Nethack/The NetHack learning environment.md","x":-280,"y":3035,"width":400,"height":400,"color":"5"},
		{"id":"01f1cad1a62a0b13","type":"file","file":"Figures/Curiosity driven exploration by self-supervised prediction Figure 2.png","x":1688,"y":1313,"width":1044,"height":560},
		{"id":"b94e2122468afc19","type":"file","file":"Papers/Exploration/RIDE. rewarding impact-driven exploration for procedurally-generated environments.md","x":1160,"y":2240,"width":400,"height":400,"color":"4"},
		{"id":"486cdc2f8d85cef2","type":"file","file":"Figures/The NetHack Learning Environment Figure 11.png","x":-716,"y":4200,"width":1273,"height":500},
		{"id":"f264d4dfde6aacaa","type":"file","file":"Figures/The NetHack Learning Environment Figure 2.png","x":-713,"y":3600,"width":736,"height":460},
		{"id":"6d27ac5ef70e49be","type":"text","text":"Noisy TV","x":1650,"y":2480,"width":250,"height":60},
		{"id":"69e8a9bf6da7400a","type":"text","text":"ICM's return drops in the environment with noisy ball, Count takes more time to achieve the same performance (probably due to needing more time to count all the new noisy states). RIDE is fine. RND???\n","x":2002,"y":2480,"width":356,"height":182},
		{"id":"1510de248b7fb72e","type":"text","text":"MiniGrid Results","x":1660,"y":2760,"width":250,"height":60},
		{"id":"ff26911fe9ecf3c2","type":"text","text":"Implementation","x":1650,"y":2240,"width":250,"height":60},
		{"id":"dc308d111b6b443d","type":"text","text":"Same as ICM. Only the current and next states + action are needed for loss computation","x":2002,"y":2240,"width":362,"height":120},
		{"id":"f9fe0ba38d7f2920","type":"file","file":"Figures/RIDE Figure 2.png","x":2520,"y":2222,"width":1041,"height":440},
		{"id":"5eeca52c2ec66952","type":"file","file":"Papers/Exploration/First return, then explore.md","x":1160,"y":3160,"width":400,"height":400,"color":"4"},
		{"id":"c58ea642467d3d18","type":"text","text":"Definitely not implementable within Sample Factory. Probably not even within any other RL freamwork (e.g. RLLIB). Would probably have to implement everything from ground up. Likely training would take too long.","x":1688,"y":3186,"width":314,"height":349},
		{"id":"9f4d94012638b877","type":"text","text":"ICM, RND and Count managed well in sparse rewards settings\n","x":2002,"y":2760,"width":259,"height":86},
		{"id":"573f356be37d3e5f","type":"text","text":"Only RIDE managed to achieve substantial return on a complex environment (MultiRoom-N12-S10)","x":2002,"y":2880,"width":259,"height":160},
		{"id":"e272846acbfb3534","type":"text","text":"Intrinsic reward analysis","x":2364,"y":2760,"width":250,"height":60},
		{"id":"093d0881667e1f18","type":"text","text":"Implementation","x":1688,"y":1970,"width":250,"height":60},
		{"id":"6c628657381ec96c","type":"text","text":"Seems implementable. The only thing required for the loss computation is current state, action and the next state. Since PPO is always batched this should be easily achievable, except I need to come up with a fix for the last state. For inspiration I would have to look at the advantage computation (the have to use some trick for the last state)","x":2047,"y":1920,"width":428,"height":220},
		{"id":"b8b408085b4e6aaa","type":"text","text":"Main challenge will be keeping track of episodic memory `M`. Need to investigate implementation of LSTM in [[Sample Factory]] ","x":2080,"y":480,"width":390,"height":130},
		{"id":"e2f11ff090fa3b86","type":"text","text":"Structure of NethHack","x":120,"y":3600,"width":250,"height":60}
	],
	"edges":[
		{"id":"aad76b6a8bcd8add","fromNode":"9cd2b466d8460512","fromSide":"right","toNode":"2dac947dacde746e","toSide":"left"},
		{"id":"9140b67f0c9cc93c","fromNode":"9cd2b466d8460512","fromSide":"left","toNode":"8ababad3874256dd","toSide":"right"},
		{"id":"5ab7538b94e2ab03","fromNode":"9cd2b466d8460512","fromSide":"top","toNode":"f6e57be8870aac37","toSide":"bottom"},
		{"id":"9fc17789768aa21e","fromNode":"9cd2b466d8460512","fromSide":"bottom","toNode":"172f0331c219c8d3","toSide":"top"},
		{"id":"0a153fd5adea595c","fromNode":"2dac947dacde746e","fromSide":"right","toNode":"9ffa4fcedb0b71ef","toSide":"left"},
		{"id":"2aa42fea8a4ccdab","fromNode":"9ffa4fcedb0b71ef","fromSide":"right","toNode":"c98890147ada15f4","toSide":"left"},
		{"id":"990635368a4826ba","fromNode":"2dac947dacde746e","fromSide":"right","toNode":"5eeca52c2ec66952","toSide":"left"},
		{"id":"32492a40e247d37c","fromNode":"5eeca52c2ec66952","fromSide":"right","toNode":"c58ea642467d3d18","toSide":"left"},
		{"id":"51d7f0fb2e78361b","fromNode":"2dac947dacde746e","fromSide":"right","toNode":"123abf0f0efe5e60","toSide":"left"},
		{"id":"b46e4829f1652c01","fromNode":"123abf0f0efe5e60","fromSide":"right","toNode":"08a8686e3b86fc83","toSide":"left"},
		{"id":"13c932d5a2ef189f","fromNode":"123abf0f0efe5e60","fromSide":"right","toNode":"7c40b754c960f28d","toSide":"left"},
		{"id":"2dbb2fa5b7499313","fromNode":"123abf0f0efe5e60","fromSide":"right","toNode":"b02a599de63de2be","toSide":"left"},
		{"id":"88c25c9298120cfa","fromNode":"7c40b754c960f28d","fromSide":"right","toNode":"b8b408085b4e6aaa","toSide":"left"},
		{"id":"916dd8a1c2a4a56e","fromNode":"08a8686e3b86fc83","fromSide":"top","toNode":"9ffa4fcedb0b71ef","toSide":"bottom"},
		{"id":"848aeafd310233dd","fromNode":"2dac947dacde746e","fromSide":"right","toNode":"95d8f63bf3911f07","toSide":"left"},
		{"id":"5ddd51a3b3efcdf4","fromNode":"95d8f63bf3911f07","fromSide":"right","toNode":"01f1cad1a62a0b13","toSide":"left"},
		{"id":"52e4f96b5a048090","fromNode":"172f0331c219c8d3","fromSide":"bottom","toNode":"0942876d50f7bde2","toSide":"top"},
		{"id":"7d7f11503ec2d5fe","fromNode":"0942876d50f7bde2","fromSide":"bottom","toNode":"486cdc2f8d85cef2","toSide":"top"},
		{"id":"c37a451f0f4c6d75","fromNode":"b94e2122468afc19","fromSide":"right","toNode":"f9fe0ba38d7f2920","toSide":"left"},
		{"id":"dee589339a62f415","fromNode":"2dac947dacde746e","fromSide":"right","toNode":"b94e2122468afc19","toSide":"left"},
		{"id":"5965113075ab726b","fromNode":"0942876d50f7bde2","fromSide":"bottom","toNode":"f264d4dfde6aacaa","toSide":"top"},
		{"id":"b36101abb64bd0d9","fromNode":"1510de248b7fb72e","fromSide":"right","toNode":"9f4d94012638b877","toSide":"left"},
		{"id":"5cb74e886467bb18","fromNode":"1510de248b7fb72e","fromSide":"right","toNode":"573f356be37d3e5f","toSide":"left"},
		{"id":"9b44a95553c3bda4","fromNode":"b94e2122468afc19","fromSide":"right","toNode":"6d27ac5ef70e49be","toSide":"left"},
		{"id":"f48ca68c758f9e7a","fromNode":"b94e2122468afc19","fromSide":"right","toNode":"1510de248b7fb72e","toSide":"left"},
		{"id":"74f0316ad5d91730","fromNode":"6d27ac5ef70e49be","fromSide":"right","toNode":"69e8a9bf6da7400a","toSide":"left"},
		{"id":"d06faaff8dae5770","fromNode":"b94e2122468afc19","fromSide":"right","toNode":"ff26911fe9ecf3c2","toSide":"left"},
		{"id":"01a7eb51a1eb8c97","fromNode":"ff26911fe9ecf3c2","fromSide":"right","toNode":"dc308d111b6b443d","toSide":"left"},
		{"id":"12d2fc44f10b9886","fromNode":"b94e2122468afc19","fromSide":"right","toNode":"e272846acbfb3534","toSide":"left"},
		{"id":"e18390dabde83a20","fromNode":"95d8f63bf3911f07","fromSide":"right","toNode":"093d0881667e1f18","toSide":"left"},
		{"id":"aefb0f7dd149e34e","fromNode":"093d0881667e1f18","fromSide":"right","toNode":"6c628657381ec96c","toSide":"left"},
		{"id":"5d2a0e2a5f154c0c","fromNode":"0942876d50f7bde2","fromSide":"bottom","toNode":"e2f11ff090fa3b86","toSide":"top"}
	]
}